{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b732c04f-9d02-4270-ba7d-1d80af596c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout, \\\n",
    "  BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d556b14-d5c7-49a0-a39e-9e0e772e6292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Load in the data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# map inputs to (-1, +1) for better training\n",
    "x_train, x_test = x_train / 255.0 * 2 - 1, x_test / 255.0 * 2 - 1\n",
    "print(\"x_train.shape:\", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "453634fa-38a6-4d44-95a8-0e0a7ec59c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the data\n",
    "N, H, W = x_train.shape\n",
    "D = H * W\n",
    "x_train = x_train.reshape(-1, D)\n",
    "x_test = x_test.reshape(-1, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e38a06a-db94-4762-8fa4-be9665da62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality of the latent space\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7619b7d7-07e5-4732-b032-b2f7e3e66086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the generator model\n",
    "def build_generator(latent_dim):\n",
    "  i = Input(shape=(latent_dim,))\n",
    "  x = Dense(256, activation=LeakyReLU(alpha=0.2))(i)\n",
    "  x = BatchNormalization(momentum=0.7)(x)\n",
    "  x = Dense(512, activation=LeakyReLU(alpha=0.2))(x)\n",
    "  x = BatchNormalization(momentum=0.7)(x)\n",
    "  x = Dense(1024, activation=LeakyReLU(alpha=0.2))(x)\n",
    "  x = BatchNormalization(momentum=0.7)(x)\n",
    "  x = Dense(D, activation='tanh')(x)\n",
    "\n",
    "  model = Model(i, x)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0cb3202-7d8b-4b10-a9ff-cc39f52da22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the discriminator model\n",
    "def build_discriminator(img_size):\n",
    "  i = Input(shape=(img_size,))\n",
    "  x = Dense(512, activation=LeakyReLU(alpha=0.2))(i)\n",
    "  x = Dense(256, activation=LeakyReLU(alpha=0.2))(x)\n",
    "  x = Dense(1, activation='sigmoid')(x)\n",
    "  model = Model(i, x)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b540ddf7-bc78-408a-b96e-194ca53fbc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Compile both models in preparation for training\n",
    "\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(D)\n",
    "discriminator.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=Adam(0.0002, 0.5),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Build and compile the combined model\n",
    "generator = build_generator(latent_dim)\n",
    "\n",
    "# Create an input to represent noise sample from latent space\n",
    "z = Input(shape=(latent_dim,))\n",
    "\n",
    "# Pass noise through generator to get an image\n",
    "img = generator(z)\n",
    "\n",
    "# Make sure only the generator is trained\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The true output is fake, but we label them real!\n",
    "fake_pred = discriminator(img)\n",
    "\n",
    "# Create the combined model object\n",
    "combined_model = Model(z, fake_pred)\n",
    "\n",
    "# Compile the combined model\n",
    "combined_model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b2bbee1-ede1-4f2e-9b33-cf9749f040af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the GAN\n",
    "\n",
    "\n",
    "# Config\n",
    "batch_size = 32\n",
    "epochs = 30000\n",
    "sample_period = 200 # every `sample_period` steps generate and save some data\n",
    "\n",
    "\n",
    "# Create batch labels to use when calling train_on_batch\n",
    "ones = np.ones(batch_size)\n",
    "zeros = np.zeros(batch_size)\n",
    "\n",
    "# Store the losses\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "# Create a folder to store generated images\n",
    "if not os.path.exists('gan_images'):\n",
    "  os.makedirs('gan_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d0a01e0-f8e2-410a-b390-5d8f037ca43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to generate a grid of random samples from the generator\n",
    "# and save them to a file\n",
    "def sample_images(epoch):\n",
    "  rows, cols = 5, 5\n",
    "  noise = np.random.randn(rows * cols, latent_dim)\n",
    "  imgs = generator.predict(noise)\n",
    "\n",
    "  # Rescale images 0 - 1\n",
    "  imgs = 0.5 * imgs + 0.5\n",
    "\n",
    "  fig, axs = plt.subplots(rows, cols)\n",
    "  idx = 0\n",
    "  for i in range(rows):\n",
    "    for j in range(cols):\n",
    "      axs[i,j].imshow(imgs[idx].reshape(H, W), cmap='gray')\n",
    "      axs[i,j].axis('off')\n",
    "      idx += 1\n",
    "  fig.savefig(\"gan_images/%d.png\" % epoch)\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d00bc74-efab-45d5-ac50-e91da0899b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'update_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m fake_imgs \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mpredict(noise)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Train the discriminator\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# both loss and accuracy are returned\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m d_loss_real, d_acc_real \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mones\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m d_loss_fake, d_acc_fake \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(fake_imgs, zeros)\n\u001b[0;32m     19\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (d_loss_real \u001b[38;5;241m+\u001b[39m d_loss_fake)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:598\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m():\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[1;32m--> 598\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    599\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:224\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution), iterator\n\u001b[0;32m    223\u001b[0m     ):\n\u001b[1;32m--> 224\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mone_step_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:110\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.one_step_on_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[0;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[0;32m    112\u001b[0m         outputs,\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[0;32m    114\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    115\u001b[0m     )\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:66\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     58\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n\u001b[0;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss(\n\u001b[0;32m     60\u001b[0m     x\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m     61\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     65\u001b[0m )\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss_tracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state\u001b[49m(\n\u001b[0;32m     67\u001b[0m     loss, sample_weight\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mshape(tree\u001b[38;5;241m.\u001b[39mflatten(x)[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     68\u001b[0m )\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mscale_loss(loss)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'update_state'"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "for epoch in range(epochs):\n",
    "  ###########################\n",
    "  ### Train discriminator ###\n",
    "  ###########################\n",
    "  \n",
    "  # Select a random batch of images\n",
    "  idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "  real_imgs = x_train[idx]\n",
    "  \n",
    "  # Generate fake images\n",
    "  noise = np.random.randn(batch_size, latent_dim)\n",
    "  fake_imgs = generator.predict(noise)\n",
    "  \n",
    "  # Train the discriminator\n",
    "  # both loss and accuracy are returned\n",
    "  d_loss_real, d_acc_real = discriminator.train_on_batch(real_imgs, ones)\n",
    "  d_loss_fake, d_acc_fake = discriminator.train_on_batch(fake_imgs, zeros)\n",
    "  d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "  d_acc  = 0.5 * (d_acc_real + d_acc_fake)\n",
    "  \n",
    "  \n",
    "  #######################\n",
    "  ### Train generator ###\n",
    "  #######################\n",
    "  \n",
    "  noise = np.random.randn(batch_size, latent_dim)\n",
    "  g_loss = combined_model.train_on_batch(noise, ones)\n",
    "  \n",
    "  # do it again!\n",
    "  noise = np.random.randn(batch_size, latent_dim)\n",
    "  g_loss = combined_model.train_on_batch(noise, ones)\n",
    "  \n",
    "  # Save the losses\n",
    "  d_losses.append(d_loss)\n",
    "  g_losses.append(g_loss)\n",
    "  \n",
    "  if epoch % 100 == 0:\n",
    "    print(f\"epoch: {epoch+1}/{epochs}, d_loss: {d_loss:.2f}, \\\n",
    "      d_acc: {d_acc:.2f}, g_loss: {g_loss:.2f}\")\n",
    "  \n",
    "  if epoch % sample_period == 0:\n",
    "    sample_images(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246c7aaa-2436-4c36-8aa2-6db8136df22c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
