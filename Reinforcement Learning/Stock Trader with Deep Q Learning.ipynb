{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a45468-e56c-4806-8f40-93f6c6a237b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import argparse\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8ba986-d9dc-44a4-ad14-f18b432d61e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    return pd.read_csv(\"data/aapl_msi_sbux.csv\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7bce7ec-2589-458b-be25-bb6cbff83486",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32) # Buffer per lo state\n",
    "        self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32) # Buffer per il \"next\" state\n",
    "        self.act_buf = np.zeros(size, dtype=np.uint8)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.uint8)\n",
    "        self.ptr = 0 # Storiamo il pointer per fare l'array circolare\n",
    "        self.size = 0 # Storiamo la current size del buffer\n",
    "        self.max_size = size # Storiamo la max size oltre il quale il buffer cancella le vecchie azioni\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs1_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size # In questo modo creiamo un buffer circolare: se si raggiunge la max size, si torna indietro\n",
    "        self.size = min(self.size + 1, self.max_size) # Settiamo la size come valore, cappando in max_size\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idx = np.random.randint(0, self.size, size=batch_size) # Seleziona \"batch_size\" valori random tra 0 e size\n",
    "        return dict(\n",
    "            s = self.obs1_buf[idx],\n",
    "            a = self.act_buf[idx],\n",
    "            r = self.rew_buf[idx],\n",
    "            s2 = self.obs2_buf[idx],\n",
    "            d = self.done_buf[idx]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e68e7896-9c3c-4f3a-9ab2-4d841acb5ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler(env):\n",
    "    \"\"\"\n",
    "    Crea uno scaler e fitta sui training states\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    for _ in range(env.n_steps):\n",
    "        action = np.random.choice(env.action_space)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        states.append(state)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    states = np.array(states).reshape(-1, state.shape[1])\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(states)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aa8628d-999b-4aa8-b288-5a77380a569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04ce8bd0-15df-474d-b936-0662859b395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(input_dim, n_actions, n_hidden_layers=1, n_hidden_dim=64):\n",
    "    i = Input(shape=(input_dim,))\n",
    "    x = i\n",
    "    for _ in range(n_hidden_layers):\n",
    "        x = Dense(n_hidden_dim, activation=\"relu\")(x)\n",
    "    x = Dense(n_actions)(x)\n",
    "    model = Model(i, x)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7795a63-473a-4004-8f14-435ec76d8376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStockEnv:\n",
    "    \"\"\"\n",
    "    Ambiente di trading 3 stocks\n",
    "    Stato: un array a 7 dimensioni che contiene:\n",
    "        - i prezzi per le tre stock\n",
    "        - il numero di shares acquistate per i tre stock\n",
    "        - i soldi contanti disponibili\n",
    "    Azioni: variabile categorica tra 0 e (3^3)-1, con i seguenti valori per stock:\n",
    "        - 0: vendi\n",
    "        - 1: tieni\n",
    "        - 2: compra\n",
    "    \"\"\"\n",
    "    def __init__(self, data, initial_investment=20_000):\n",
    "        self.stock_price_history = data\n",
    "        self.n_steps = self.stock_price_history.shape[0]\n",
    "        self.n_stocks = self.stock_price_history.shape[1]\n",
    "\n",
    "        self.initial_investment = initial_investment\n",
    "        self.cur_step = None\n",
    "        self.owned_stocks = None\n",
    "        self.stock_prices = None\n",
    "        self.cash_in_hands = None\n",
    "        \n",
    "        self.action_space = np.arange(3**n_stocks) # numeri tra 0 e 26\n",
    "\n",
    "        # Creiamo le permutazioni di azioni come gruppi da tre contenenti i numeri [0, 1, 2]\n",
    "        # Sto costrutto mappa una lista di tuple in una lista di liste. La lista di tuple è generata da Itertools\n",
    "        self.action_list = list(map(list, itertools.product([0, 1, 2], repeat=3)))\n",
    "\n",
    "        self.state_dim = self.n_stocks * 2 + 1 # Dimensione dello stato\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.cash_in_hands = self.initial_investment\n",
    "        self.owned_stocks = np.zeros(self.n_stocks)\n",
    "        self.stock_prices = self.stock_price_history[self.cur_step]\n",
    "        return self._get_obs() # Ritorna lo stato iniziale\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in self.action_space\n",
    "\n",
    "        prev_val = self._get_val()\n",
    "        self.cur_step +=1\n",
    "        self.stock_prices = self.stock_price_history[self.cur_step]\n",
    "\n",
    "        self._trade(action)\n",
    "        \n",
    "        cur_val = self._get_val()\n",
    "        reward = cur_val - prev_val\n",
    "        done = self.cur_step == self.n_steps-1\n",
    "        info = {\"cur_val\": cur_val}\n",
    "\n",
    "        return self._get_obs(), reward, done, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        price = self.stock_prices.reshape(1, -1)\n",
    "        own = self.owned_stocks.reshape(1, -1)\n",
    "        hands = np.array(self.cash_in_hands).reshape(1, -1)\n",
    "        obs = np.c_[own, price, hands]\n",
    "        return obs\n",
    "\n",
    "    def _get_val(self):\n",
    "        return self.owned_stocks.dot(self.stock_prices) + self.cash_in_hands # Somma il valore le stock in mano + i soldi\n",
    "\n",
    "    def _trade(self, action):\n",
    "        # Vendiamo prima le stock, poi compriamo in round robin quelle da comprare\n",
    "        # L'azione qui è un numero tra 0 e 26.\n",
    "        # Campiona un vettore di azioni tra la lista di permutazioni\n",
    "        # Il significato è, ad esempio, [2, 0, 1] -> Compra stock 0, Vendi stock 1, Tieni stock 2\n",
    "        action_vec = np.array(self.action_list[action]).reshape(-1,)\n",
    "        sell_idx = np.where(action_vec==0)[0]\n",
    "        buy_idx = np.where(action_vec==2)[0]\n",
    "\n",
    "\n",
    "        # Vende tutti gli stock contrassegnati da sell\n",
    "        for idx in sell_idx:\n",
    "            stock_count = self.owned_stocks[idx]\n",
    "            stock_value = self.stock_prices[idx] * stock_count\n",
    "            self.owned_stocks[idx] = 0\n",
    "            self.cash_in_hands += stock_value\n",
    "\n",
    "        # Compra uno stock per volta in \"round robin\" per tutti gli stock contrassegnati da buy fino a esaurimento fondi\n",
    "        if len(buy_idx) > 0:\n",
    "            min_price = min(self.stock_prices[buy_idx])\n",
    "            while self.cash_in_hands >= min_price:\n",
    "                for idx in buy_idx:\n",
    "                    price = self.stock_prices[idx]\n",
    "                    if self.cash_in_hands >= price:\n",
    "                        self.owned_stocks[idx] += 1\n",
    "                        self.cash_in_hands -= price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49c9acdb-3952-4ba8-9842-94b68811f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = ReplayBuffer(state_size, action_size, size=500)\n",
    "        self.gamma = 0.9 # fattore di sconto per i reward futuri\n",
    "        self.epsilon = 1 # Epsilon per esplorazione (inizia da 1)\n",
    "        self.epsilon_min = 0.01 # Epsilon minimo da raggiungere\n",
    "        self.epsilon_decay = 0.995 # Fattore di decadimento Epsilon\n",
    "        self.model = mlp(state_size, action_size) # La rete neurale\n",
    "\n",
    "    def update_replay_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.store(state, action, reward, next_state, done)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "            \n",
    "        act_values = self.model.predict(state, verbose=0) # Ritorna il Q value di tutte le azioni.\n",
    "        return np.argmax(act_values) # Ritorna l'indice dell'azione a valore maggiore\n",
    "\n",
    "    def replay(self, batch_size=32):\n",
    "        # Check se ci sono abbastanza samples\n",
    "        if self.memory.size < batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = self.memory.sample_batch(batch_size)\n",
    "        # Il minibatch è un dizionario con array di dimensione \"batch_size\"\n",
    "        states = minibatch[\"s\"]\n",
    "        actions = minibatch[\"a\"]\n",
    "        rewards = minibatch[\"r\"]\n",
    "        next_states = minibatch[\"s2\"]\n",
    "        done = minibatch[\"d\"] # -> Array di True/False\n",
    "\n",
    "        # Piccolo reminder: Nel reinforcement learning (Q learning) sia la predizione che il target sono previsti dal modello\n",
    "        # La differenza sta nel fatto che il target si calcola come valore massimo delle previsioni sul prossimo stato\n",
    "        # Predizione: y = Q(s, a) -> su tutte le azioni\n",
    "        # Target:  y_hat = r + gamma * max_a'(Q(s', a')) -> sull'azione presa\n",
    "        targets = rewards + self.gamma * np.max(self.model.predict(next_states, verbose=0), axis=1)\n",
    "        # Se il flag done è True, sistemiamo il reward affinchè non abbia \"future rewards\"\n",
    "        targets[done] = rewards[done] # Qui cambiamo solo i valori laddove done = True\n",
    "\n",
    "        # Ora, con Keras dobbiamo assicurarci che targets e model predictions abbiano la stessa shape\n",
    "        # Ma noi vogliamo fare l'update dei valori solo sull'azione intrapresa\n",
    "        # La nostra predizione avrà shape batch x actions ma ora i targets sono batch x 1\n",
    "        # Per evitare l'update, semplicemente creiamo dei target fasulli pari alle predizioni, così che la loss sia 0\n",
    "        target_full = self.model.predict(states, verbose=0)\n",
    "        target_full[np.arange(batch_size), actions] = targets # All'indice dell'azione inseriamo il vero valore, per ogni batch\n",
    "\n",
    "        self.model.train_on_batch(states, target_full)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e55ccd7-423b-46ca-b1e5-abf7c0d0f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_episode(agent, env, is_train):\n",
    "    state = env.reset()\n",
    "    state = scaler.transform(state) # 1xD state\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = scaler.transform(next_state)\n",
    "        if is_train == True:\n",
    "            agent.update_replay_memory(state, action, reward, next_state, done)\n",
    "            if env.cur_step % 40 == 0: \n",
    "                agent.replay(batch_size)\n",
    "        state = next_state\n",
    "\n",
    "    return info[\"cur_val\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c4d5697-258c-4880-9415-c1e4d5880463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "models_folder = \"models\"\n",
    "rewards_folder = \"rewards\"\n",
    "num_episodes = 2000\n",
    "batch_size = 32\n",
    "initial_investment = 20000\n",
    "\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-m\", \"--mode\", type=bool, required=True, help=\"either True or False\")\n",
    "args = parser.parse_args()\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "909e4a9c-8852-4366-916d-b78f4413727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dir(models_folder)\n",
    "make_dir(rewards_folder)\n",
    "data = get_data()\n",
    "n_timesteps, n_stocks = data.shape\n",
    "n_train = n_timesteps // 2\n",
    "train_data = data[:n_train]\n",
    "test_data = data[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a38685a-1d3d-42bc-b84e-5da7e7b78b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,755</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m512\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m)                  │           \u001b[38;5;34m1,755\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,267</span> (8.86 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,267\u001b[0m (8.86 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,267</span> (8.86 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,267\u001b[0m (8.86 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "Episode 1/2000: end episode value -> 28009.88619999897 | duration -> 0:00:02.964501\n",
      "Episode 2/2000: end episode value -> 31257.530000001057 | duration -> 0:00:05.398296\n",
      "Episode 3/2000: end episode value -> 35086.884299999816 | duration -> 0:00:07.785205\n",
      "Episode 4/2000: end episode value -> 27889.732899999115 | duration -> 0:00:08.391955\n",
      "Episode 5/2000: end episode value -> 31866.45340000168 | duration -> 0:00:11.020333\n",
      "Episode 6/2000: end episode value -> 27510.10500000112 | duration -> 0:00:11.226567\n",
      "Episode 7/2000: end episode value -> 20650.677799998546 | duration -> 0:00:14.738201\n",
      "Episode 8/2000: end episode value -> 28623.10029999592 | duration -> 0:00:15.521881\n",
      "Episode 9/2000: end episode value -> 32591.927800000034 | duration -> 0:00:18.370156\n",
      "Episode 10/2000: end episode value -> 23908.13499999946 | duration -> 0:00:16.697589\n",
      "Episode 11/2000: end episode value -> 19406.158299999825 | duration -> 0:00:17.453248\n",
      "Episode 12/2000: end episode value -> 24422.123999999098 | duration -> 0:00:19.764310\n",
      "Episode 13/2000: end episode value -> 24601.764700000174 | duration -> 0:00:19.979326\n",
      "Episode 14/2000: end episode value -> 26946.28359999961 | duration -> 0:00:21.343140\n",
      "Episode 15/2000: end episode value -> 25924.402700001596 | duration -> 0:00:22.715035\n",
      "Episode 16/2000: end episode value -> 22592.909300000218 | duration -> 0:00:23.536068\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m     11\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m---> 12\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mplay_one_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     dt \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: end episode value -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | duration -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m, in \u001b[0;36mplay_one_episode\u001b[1;34m(agent, env, is_train)\u001b[0m\n\u001b[0;32m      4\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m----> 7\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     next_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m      9\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(next_state)\n",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m, in \u001b[0;36mDQNAgent.act\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_size)\n\u001b[0;32m     19\u001b[0m act_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(state, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# Ritorna il Q value di tutte le azioni.\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mact_values\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1136\u001b[0m, in \u001b[0;36m_argmax_dispatcher\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;124;03m    Returns the indices that would sort an array.\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \n\u001b[0;32m   1132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margsort\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, kind\u001b[38;5;241m=\u001b[39mkind, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[1;32m-> 1136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_argmax_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   1137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argmax_dispatcher)\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = MultiStockEnv(train_data, initial_investment)\n",
    "state_size = env.state_dim\n",
    "action_size = len(env.action_space)\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "scaler = get_scaler(env)\n",
    "\n",
    "portfolio_value = []\n",
    "\n",
    "print(\"Starting\")\n",
    "for e in range(num_episodes):\n",
    "    t0 = datetime.now()\n",
    "    val = play_one_episode(agent, env, is_train=True)\n",
    "    dt = datetime.now() - t0\n",
    "    print(f\"Episode {e+1}/{num_episodes}: end episode value -> {val} | duration -> {dt}\")\n",
    "    portfolio_value.append(val)\n",
    "\n",
    "agent.save(f\"{models_folder}/dqn.h5\")\n",
    "\n",
    "with open(f\"{models_folder}/scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "np.save(f\"{rewards_folder}/Train.npy\", portfolio_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e78176-1a91-47e2-90c5-be53155bfbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "portfolio_value = []\n",
    "env = MultiStockEnv(test_data, initial_investment)\n",
    "agent.epsilon\n",
    "for e in range(num_episodes):\n",
    "    t0 = datetime.now()\n",
    "    val = play_one_episode(agent, env, is_train=False)\n",
    "    dt = datetime.now() - t0\n",
    "    print(f\"Episode {e+1}/{num_episodes}: end episode value -> {val} | duration -> {dt}\")\n",
    "    portfolio_value.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02daa891-c852-4287-93e9-b48874823cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c4a0cb-eb48-490a-8293-163a83ab843c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
